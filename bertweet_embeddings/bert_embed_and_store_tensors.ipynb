{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_arabic_chars(text):\n",
    "    return re.sub(\"[\\p{Arabic}\\s]\", \" \", text)\n",
    "    #return re.sub(r'[^0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(mode + '_china_full_nort.csv')\n",
    "tweets = tweets.dropna() # some rows come in as blank so they need to be dropped\n",
    "tweets.text = tweets.text.apply(remove_arabic_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tokenize_and_embed(tweets, batch_size = 200):\n",
    "    \n",
    "    embeddings = torch.Tensor()\n",
    "    \n",
    "    for i in range(0, len(tweets), batch_size):\n",
    "        batch = tweets[i : min(len(tweets), i+batch_size)]\n",
    "        print(\"Processing chunk \" + str(i) + \" to \" + str(i + len(batch)))\n",
    "        #print(batch[0])\n",
    "        tokens = tokenizer(batch, padding='max_length', max_length=130, truncation=True, add_special_tokens = False, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bertweet(**tokens)\n",
    "    \n",
    "        embeddings = torch.cat((embeddings, outputs.pooler_output)) #pooler_output is an embedding for the entire tweet\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(tweets, chunk_size=1000):\n",
    "    \n",
    "    for i in range(0, len(tweets), chunk_size):\n",
    "        batch = tweets[i : min(len(tweets), i+chunk_size)]\n",
    "        filename = \"embeddings/\" + mode + \"_china_embedding_\" + str(i) + \".pt\"\n",
    "        torch.save(batch_tokenize_and_embed(batch), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 800\n",
      "Processing chunk 800 to 1000\n",
      "Processing chunk 0 to 200\n",
      "Processing chunk 200 to 400\n",
      "Processing chunk 400 to 600\n",
      "Processing chunk 600 to 733\n"
     ]
    }
   ],
   "source": [
    "embed_and_store(list(tweets.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3403,  0.2427,  0.0204,  ...,  0.1723, -0.0912, -0.0112],\n",
       "         [ 0.4755,  0.0509,  0.3595,  ...,  0.4801,  0.4298, -0.5607],\n",
       "         [ 0.0320,  0.0301,  0.1053,  ...,  0.3057, -0.2156,  0.2590],\n",
       "         ...,\n",
       "         [-0.3403,  0.2427,  0.0204,  ...,  0.1723, -0.0912, -0.0112],\n",
       "         [-0.3403,  0.2427,  0.0204,  ...,  0.1723, -0.0912, -0.0112],\n",
       "         [-0.3403,  0.2427,  0.0204,  ...,  0.1723, -0.0912, -0.0112]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 3.6026e-01, -2.0943e-01, -6.9925e-02, -1.5168e-01,  3.9827e-02,\n",
       "          8.7728e-03,  1.8579e-01, -2.2495e-01,  1.7258e-01, -1.2914e-01,\n",
       "         -1.7045e-01, -1.2448e-01, -1.9424e-01,  4.6071e-02,  2.5059e-01,\n",
       "         -6.9921e-02, -1.4975e-01,  1.6202e-02, -4.7156e-02,  1.3567e-01,\n",
       "         -1.3274e-01, -1.1450e-01,  3.6526e-01,  1.2711e-02, -7.4361e-03,\n",
       "          8.4331e-02, -2.5667e-01,  6.6397e-02,  3.4457e-02, -1.8348e-02,\n",
       "          1.2330e-01, -2.1634e-01,  3.2038e-02,  1.9610e-01,  1.8525e-01,\n",
       "         -2.9740e-01,  1.0215e-01,  1.6341e-01, -9.3947e-02, -3.4117e-02,\n",
       "         -3.2576e-02, -1.1660e-01, -6.5930e-02, -1.6810e-01, -3.6264e-02,\n",
       "         -3.1603e-02, -1.0175e-01,  3.8387e-02,  3.4393e-01, -6.6573e-03,\n",
       "         -5.3838e-02,  7.8337e-02, -1.9323e-01,  7.4526e-02, -6.6544e-02,\n",
       "          1.8050e-01,  8.4144e-02,  1.4241e-02, -4.2630e-02,  1.3711e-01,\n",
       "         -3.6733e-02,  3.0741e-01, -3.0429e-01, -3.7267e-01, -1.0907e-01,\n",
       "         -1.1681e-01, -5.1544e-02,  1.1551e-01,  3.4186e-01,  3.0149e-01,\n",
       "          2.3928e-01,  1.0017e-01, -3.2472e-01, -3.1744e-02, -6.4053e-02,\n",
       "          4.6701e-03,  2.6243e-02, -1.2010e-02,  1.3752e-01,  1.3379e-01,\n",
       "         -5.4141e-02, -2.3799e-01,  1.8938e-01, -1.7388e-01, -2.9248e-01,\n",
       "          7.2938e-02,  1.5566e-01,  1.9403e-01, -4.9249e-02,  3.1252e-01,\n",
       "          1.1360e-01,  1.2947e-01, -1.7563e-01, -1.6285e-01, -1.1655e-01,\n",
       "         -1.4597e-01,  9.6984e-02,  1.9576e-01, -5.0336e-02, -1.0448e-01,\n",
       "         -2.5398e-01,  1.5163e-01, -8.5000e-02,  1.1643e-01,  3.4441e-02,\n",
       "         -3.0323e-02, -4.1737e-04,  1.8624e-01,  9.2052e-02, -3.3938e-01,\n",
       "         -3.5091e-03, -1.1057e-01, -2.1692e-01, -1.3855e-01,  1.7909e-01,\n",
       "         -3.1472e-02, -2.1177e-01,  1.6222e-01,  2.2376e-01, -5.8881e-02,\n",
       "         -1.9432e-02,  2.0620e-01,  1.5462e-01,  1.0405e-01,  8.5743e-02,\n",
       "         -9.5409e-02, -4.8514e-02, -7.8356e-03,  8.6897e-02, -1.5365e-01,\n",
       "          4.1081e-02, -1.2879e-01,  2.2398e-01, -1.8535e-01,  1.5502e-01,\n",
       "         -1.7378e-02,  2.4017e-01, -4.6547e-02,  1.9413e-01,  1.5646e-01,\n",
       "          9.4078e-02, -2.0473e-01,  9.0157e-02,  3.4574e-01, -1.3092e-01,\n",
       "          1.0470e-01, -1.7939e-01,  4.5219e-02, -8.0517e-02, -5.2650e-02,\n",
       "         -1.0662e-01,  2.4256e-02, -2.0691e-01,  1.1904e-01, -6.4722e-02,\n",
       "         -7.5698e-02, -1.1402e-01,  1.7981e-01, -2.2860e-02, -1.7746e-01,\n",
       "          2.2859e-01, -2.7505e-01,  1.3193e-01, -7.9709e-02,  1.2893e-01,\n",
       "          2.3047e-02, -9.8281e-02, -1.4902e-01, -1.5248e-01, -8.2976e-02,\n",
       "         -1.3091e-01, -5.8006e-02, -6.2654e-01, -3.0784e-01,  5.5690e-02,\n",
       "          2.2313e-01, -1.6369e-01,  1.4013e-01,  2.3620e-01,  1.2886e-01,\n",
       "          4.9274e-02,  2.4531e-01, -4.4423e-02, -3.8489e-02, -6.5627e-02,\n",
       "          2.5701e-01, -5.5781e-02, -4.6373e-02, -6.6317e-02,  1.1524e-01,\n",
       "          4.2404e-02, -9.6000e-02,  3.1039e-01,  1.3570e-01, -3.8392e-01,\n",
       "         -2.1836e-02,  2.0236e-01, -3.3795e-01,  4.7893e-02,  1.0180e-02,\n",
       "          2.0232e-01,  1.9732e-01, -5.8981e-02,  1.0157e-01,  1.3508e-01,\n",
       "          2.2713e-02, -1.1314e-01, -5.7136e-03,  5.5418e-02, -5.3219e-02,\n",
       "         -5.3627e-02, -5.2011e-02, -1.7896e-01, -2.2934e-01, -2.0171e-01,\n",
       "          2.2169e-01, -1.6783e-01, -5.9154e-02, -5.5067e-01,  3.8541e-01,\n",
       "          1.2754e-01, -2.2766e-01,  2.2116e-01, -7.8940e-02, -7.2999e-02,\n",
       "         -2.0638e-02,  3.5598e-01,  7.3612e-02, -2.4110e-01, -2.5164e-01,\n",
       "         -7.4516e-02,  1.8398e-02,  1.3104e-01, -6.0905e-02,  1.9246e-01,\n",
       "          5.3647e-01,  1.8449e-01,  1.7731e-01, -1.2097e-03,  4.8046e-02,\n",
       "         -1.5582e-01, -5.7556e-02,  9.7550e-02, -8.0942e-02,  2.5869e-01,\n",
       "         -4.3448e-03,  4.5288e-01,  9.7766e-02, -2.8490e-01,  4.7751e-02,\n",
       "         -2.3531e-01, -2.2273e-01, -2.4961e-01,  5.4810e-02, -1.1553e-01,\n",
       "         -5.8624e-02,  1.4918e-01, -1.4892e-01,  3.8377e-01,  1.4910e-01,\n",
       "          1.2564e-01,  1.2265e-01, -3.1476e-01, -5.7261e-02, -2.2139e-01,\n",
       "          5.6235e-02,  1.5708e-01, -1.8036e-01,  6.6492e-02, -1.6742e-01,\n",
       "         -3.1724e-01, -1.5030e-01,  6.6668e-02, -1.5487e-01, -1.1149e-01,\n",
       "         -3.2019e-01, -8.0707e-02,  2.3226e-01, -3.4049e-03, -2.7791e-01,\n",
       "          8.3121e-02,  8.7300e-02, -1.0886e-01, -8.9433e-02, -4.8658e-02,\n",
       "          1.0811e-01,  8.3721e-02,  2.5914e-01,  1.3047e-01, -2.4264e-02,\n",
       "         -1.3369e-01, -1.2848e-01,  1.3279e-02,  2.4622e-01,  3.8895e-02,\n",
       "         -1.8120e-01,  1.5134e-01, -4.6039e-03,  8.1728e-02,  1.0689e-01,\n",
       "         -2.3794e-01, -1.3631e-01,  2.1542e-01, -1.9756e-01, -8.3404e-02,\n",
       "         -2.1304e-01, -6.3557e-02, -1.4535e-01,  1.9129e-01,  4.1739e-03,\n",
       "          2.0855e-01, -4.9560e-02,  1.6841e-01,  2.1934e-01,  6.5783e-02,\n",
       "          2.6305e-01,  1.3081e-02,  9.6045e-02,  2.2005e-02, -4.4350e-02,\n",
       "         -1.2326e-01, -2.8477e-01, -5.4918e-02,  4.7439e-02, -7.2934e-02,\n",
       "         -1.8916e-02, -1.2152e-01, -1.1239e-01, -7.1187e-02,  3.2208e-01,\n",
       "          3.2917e-01,  2.1620e-02, -7.6853e-02, -1.5196e-01, -1.9564e-01,\n",
       "         -7.5842e-02,  8.2791e-02,  3.3257e-02,  8.8294e-02,  3.8175e-02,\n",
       "         -3.6008e-01, -8.6934e-02,  1.8662e-01,  2.2079e-01,  1.1413e-01,\n",
       "          5.0245e-01, -5.3652e-02,  6.6170e-02, -1.8782e-01,  9.6795e-03,\n",
       "         -2.4541e-02, -5.3487e-02,  5.0875e-03, -5.0035e-02,  2.3200e-01,\n",
       "         -1.6366e-01,  1.2262e-01, -5.2823e-02, -9.1184e-02,  9.4206e-02,\n",
       "         -2.1350e-01,  1.5137e-01,  5.4105e-02, -1.3087e-01, -1.5961e-01,\n",
       "          2.4575e-01, -1.3430e-01, -1.7910e-01,  1.3721e-01, -2.8606e-02,\n",
       "         -5.7602e-02,  2.6567e-02,  2.9923e-01, -1.2480e-01, -9.6435e-02,\n",
       "          3.4923e-02, -9.4123e-02, -2.0797e-01, -2.6564e-02,  1.1179e-01,\n",
       "          1.2754e-01, -1.2229e-01, -2.3554e-01,  1.1157e-01, -3.0806e-02,\n",
       "          2.7536e-02, -3.8515e-01,  1.3062e-02,  2.3789e-01, -6.4921e-02,\n",
       "          3.5676e-01, -4.2914e-02, -4.1806e-02, -4.8477e-01,  1.5515e-01,\n",
       "          1.3748e-01,  8.0525e-02,  1.0069e-01,  1.7030e-01, -1.4703e-01,\n",
       "         -1.2327e-01, -2.3487e-01,  1.8189e-01, -7.5420e-02, -1.2447e-01,\n",
       "         -1.5064e-01,  4.1316e-02,  4.1938e-02,  1.1071e-01,  6.7768e-02,\n",
       "         -1.7181e-01,  2.3234e-01, -3.2043e-01, -6.9556e-02,  3.6098e-02,\n",
       "          4.1659e-02,  1.9277e-01,  1.0400e-01,  4.7868e-02, -2.6056e-01,\n",
       "          4.4518e-02, -1.5395e-02,  1.8704e-03,  6.5948e-02, -9.0238e-02,\n",
       "          9.1444e-03, -2.2344e-01, -1.5776e-01, -2.0240e-01, -1.4293e-01,\n",
       "         -1.3118e-01, -3.0610e-01, -7.3968e-02, -7.6698e-02, -5.0014e-02,\n",
       "         -7.4336e-02,  6.8709e-02,  1.9141e-01,  1.8014e-01, -2.9649e-01,\n",
       "         -1.7936e-01, -3.4492e-01,  1.9507e-01, -3.0390e-01, -1.3543e-01,\n",
       "         -1.6598e-01, -2.3630e-01, -2.1333e-01, -2.7021e-01, -1.5154e-01,\n",
       "          3.3272e-01, -1.3507e-01, -4.7083e-02, -2.2071e-02,  7.1592e-02,\n",
       "         -1.1588e-01,  3.3417e-01, -1.3257e-01,  1.4596e-01,  1.2431e-01,\n",
       "          1.4135e-01, -2.0724e-02,  9.8239e-02,  3.1020e-01, -1.0863e-01,\n",
       "         -2.0968e-01,  3.3016e-02, -7.6414e-02, -4.4423e-02,  1.6892e-01,\n",
       "          8.5591e-02, -2.3796e-01, -8.3237e-02,  6.1573e-02, -3.0660e-01,\n",
       "         -1.0073e-01,  2.7582e-02,  2.3384e-01,  1.6212e-01,  5.3859e-02,\n",
       "         -2.2716e-01, -4.6102e-02, -8.7658e-02,  2.6764e-01, -2.4703e-01,\n",
       "         -5.8238e-03, -2.1200e-01, -1.8252e-01,  2.5226e-01, -1.8964e-01,\n",
       "          9.9693e-02, -6.9532e-02, -2.6450e-01,  3.0858e-01, -6.1458e-02,\n",
       "         -2.6855e-01,  3.5553e-01,  5.7745e-02, -5.5945e-02,  5.9961e-03,\n",
       "         -9.5405e-02, -1.2287e-01,  1.0825e-01,  6.8422e-02, -2.8032e-01,\n",
       "         -2.9396e-01, -3.5323e-01,  1.7586e-01, -2.6505e-01,  3.7188e-01,\n",
       "          1.0534e-01, -1.4543e-02, -6.8107e-02,  5.9689e-02, -1.5411e-01,\n",
       "          1.0867e-01, -1.0063e-01, -1.6135e-01,  9.6888e-02, -4.4154e-02,\n",
       "         -1.2353e-01, -1.8348e-01, -1.9156e-01,  1.4713e-01,  1.6530e-01,\n",
       "          2.2887e-01, -1.4178e-01,  1.5118e-02, -1.3587e-01, -1.0734e-01,\n",
       "          2.7695e-01,  3.5479e-02,  1.7811e-01,  1.1922e-01,  2.5428e-01,\n",
       "         -3.4900e-01, -8.8296e-02, -3.7774e-02, -5.9429e-02, -7.3223e-02,\n",
       "         -1.4911e-01,  3.6015e-01,  6.6788e-02, -7.2604e-02, -9.4030e-03,\n",
       "          1.1159e-01,  2.4567e-02, -1.1892e-01,  2.1722e-01,  6.2161e-02,\n",
       "          1.4093e-01, -1.1121e-01,  6.6337e-02, -2.2599e-01, -8.6333e-02,\n",
       "         -3.1744e-01,  6.0485e-01, -2.6455e-01,  7.6408e-02,  9.8473e-02,\n",
       "         -7.8660e-02,  2.7859e-01, -6.9405e-02,  6.3925e-02,  3.8115e-02,\n",
       "          1.7551e-02,  1.3814e-01, -3.7574e-01, -1.0541e-01,  8.6460e-02,\n",
       "          2.4394e-01, -1.0903e-01, -2.8940e-01, -3.3187e-02, -2.4005e-01,\n",
       "         -4.2325e-01,  2.3511e-01,  8.2923e-02, -1.6703e-01, -2.0019e-01,\n",
       "          5.2947e-02,  6.2351e-02, -4.2642e-02, -1.4201e-01, -7.2192e-02,\n",
       "          1.6397e-01, -2.5453e-02, -2.9390e-01,  3.0054e-02, -1.8706e-01,\n",
       "         -9.4176e-02, -3.3517e-01,  2.9474e-01,  1.3415e-01,  1.7445e-01,\n",
       "          9.3860e-02, -1.0722e-01, -2.0318e-01,  3.7805e-02,  2.0863e-02,\n",
       "          5.3146e-01, -8.7924e-02,  2.9580e-01, -8.0481e-02, -1.5982e-01,\n",
       "         -5.5395e-03,  8.3117e-02,  2.2010e-01,  7.6846e-02, -2.2903e-01,\n",
       "          1.0742e-01,  7.8274e-02,  7.2917e-02, -1.6941e-01,  6.3162e-02,\n",
       "          3.2970e-01, -6.5008e-02,  2.3946e-01,  2.3257e-02, -4.0577e-02,\n",
       "         -7.1579e-02,  1.2769e-01,  2.2193e-01, -2.8255e-01,  1.9481e-01,\n",
       "          1.4346e-01,  7.5769e-03, -1.0128e-01,  1.6280e-01, -1.3038e-01,\n",
       "          1.3557e-01,  9.0356e-02,  9.0849e-02, -1.7289e-01,  1.1350e-01,\n",
       "          1.1433e-01, -8.7837e-02, -2.4861e-01, -3.1240e-02, -5.9840e-02,\n",
       "         -1.4662e-01,  1.5540e-02, -3.6625e-01, -2.8264e-01, -1.6113e-01,\n",
       "          1.3969e-01, -1.2934e-01, -5.0094e-03, -2.6206e-02,  4.2886e-02,\n",
       "         -1.6947e-01, -8.5123e-02, -4.3645e-02,  5.9938e-02,  5.7337e-02,\n",
       "          7.7942e-02,  3.3011e-02,  2.0385e-01,  2.3084e-01,  2.6743e-01,\n",
       "         -1.6464e-01, -8.4043e-02, -2.5357e-01,  5.2420e-03, -2.1614e-01,\n",
       "         -4.5295e-02,  2.0265e-01,  8.5130e-02,  2.7805e-01,  2.2608e-01,\n",
       "          2.0298e-01,  1.2382e-01,  1.6154e-01, -9.9306e-02, -1.1053e-01,\n",
       "          6.2063e-02, -2.5415e-01,  1.9526e-01, -1.7561e-01, -2.6902e-01,\n",
       "          2.1236e-01, -1.5557e-01,  4.7593e-02,  2.0053e-01, -9.2362e-02,\n",
       "         -1.3507e-01,  2.4054e-02, -1.0045e-01,  2.7421e-01,  2.4850e-01,\n",
       "          1.4617e-01, -9.4818e-02, -1.6770e-02,  3.6431e-02, -2.3186e-01,\n",
       "          1.3681e-02, -7.1565e-03,  2.2089e-03,  1.1961e-02, -1.6510e-01,\n",
       "         -7.9507e-02, -1.9589e-01,  1.8203e-01,  2.0413e-01, -1.0215e-02,\n",
       "         -1.8369e-01,  1.6230e-02,  1.7154e-01,  1.1909e-01,  3.5302e-01,\n",
       "          5.2697e-02,  9.0162e-02, -1.7402e-01, -2.5550e-01, -1.1515e-01,\n",
       "          3.2645e-01, -8.9939e-02, -3.1805e-01, -7.3732e-02, -1.3914e-01,\n",
       "         -1.3717e-01, -1.0386e-01,  3.5934e-01, -4.2510e-02,  1.9526e-01,\n",
       "         -2.5168e-03, -1.1326e-01, -2.1019e-01, -3.2349e-02, -2.7618e-01,\n",
       "         -1.8808e-01,  2.2269e-01,  3.4119e-01, -1.0080e-01,  9.2528e-02,\n",
       "         -1.0972e-01, -1.5610e-01,  1.1215e-01, -1.5467e-01, -7.8956e-02,\n",
       "         -9.7884e-02, -1.0677e-02, -1.4969e-01,  4.7339e-02, -7.1517e-02,\n",
       "          1.7143e-02,  9.2604e-02,  2.4026e-01, -2.6850e-01,  2.7497e-01,\n",
       "         -2.0833e-01, -1.5130e-01,  1.0732e-01,  3.4252e-01, -3.8302e-02,\n",
       "         -5.8162e-02,  5.4059e-02,  9.9280e-02, -1.3017e-01, -2.6887e-01,\n",
       "         -1.7577e-01,  2.3033e-01,  1.4902e-01,  2.5415e-01, -4.5317e-01,\n",
       "         -1.3271e-01, -1.2165e-01, -1.8437e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer([list(tweets.text)[25436]], padding='max_length', add_special_tokens=True, max_length=130, return_tensors=\"pt\")\n",
    "bertweet(**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#ISLAM IS RESPECTED IN #CHINA.  71K VIEWS SO FAR.  PLEASE SHARE, THANK YOU!  #BREAKING #Muslim #Muslims #salamaleikum #MuslimsinChina #HumanRights  #     #      #         #        #      _      #       #      _     _     _    _        #         #    _      #    _   _    https://t.co/PHm4zYiJyk'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tweets.text)[25436]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A journey along the Chinese border â€“ Ili https://t.co/7kPkbOAzZ4 https://t.co/zSuFYhWTM4'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tweets.text)[1234]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
