{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('pro_china_full.csv')\n",
    "tweets = tweets.dropna().reset_index() # some rows come in as blank so they need to be dropped - also need to reset index so they can match embeddings later\n",
    "\n",
    "train, test = train_test_split(tweets, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell tests for invalid or blank text entries and prints their index if one comes up\n",
    "\n",
    "for idx, tweet in enumerate(tweets['text']):\n",
    "    try: \n",
    "        tokenizer(tweet, padding='max_length', max_length=130, return_tensors=\"pt\")\n",
    "    except:\n",
    "        print(idx, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38646, 768]) (38646, 4)\n"
     ]
    }
   ],
   "source": [
    "# reading in tensors from file\n",
    "\n",
    "embeddings = torch.Tensor()\n",
    "\n",
    "for i in range(39):\n",
    "    filename = \"embeddings/pro_china_embedding_\" + str(i*1000) + \".pt\"\n",
    "    embeddings = torch.cat((embeddings, torch.load(filename)))\n",
    "    \n",
    "print(embeddings.shape, tweets.shape) # these should be in agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30916, 768) (30916,)\n"
     ]
    }
   ],
   "source": [
    "X_train = embeddings[train.index].detach().numpy()\n",
    "y_train = train['is_ccp']\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.Sequential()\n",
    "\n",
    "# model.add(keras.layers.Dense(512, \n",
    "#                              activation = 'relu',\n",
    "#                              kernel_regularizer=keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "#                              bias_regularizer=keras.regularizers.l2(1e-4),\n",
    "#                              activity_regularizer=keras.regularizers.l2(1e-5)))\n",
    "# model.add(keras.layers.Dropout(0.1))\n",
    "# model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# # adam optimizer is a fancier version of gradient descent.  You can read more about it here: https://arxiv.org/pdf/1412.6980.pdf\n",
    "# optimizer = keras.optimizers.Adam(clipvalue=1)\n",
    "\n",
    "# model.compile(optimizer=optimizer,\n",
    "#               loss='binary_crossentropy',  # From information theory notebooks.\n",
    "#               metrics=['accuracy'])        # What metric to output as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "#model.add(keras.layers.Normalization())\n",
    "\n",
    "model.add(keras.layers.Dense(4, \n",
    "                             activation = 'relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "#optimizer = keras.optimizers.Adam(clipvalue=1, learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "967/967 [==============================] - 3s 1ms/step - loss: 0.4244 - accuracy: 0.8315\n",
      "Epoch 2/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2969 - accuracy: 0.8794\n",
      "Epoch 3/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2684 - accuracy: 0.8925\n",
      "Epoch 4/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2508 - accuracy: 0.9016\n",
      "Epoch 5/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2404 - accuracy: 0.9070\n",
      "Epoch 6/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2325 - accuracy: 0.9095\n",
      "Epoch 7/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2264 - accuracy: 0.9121\n",
      "Epoch 8/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2215 - accuracy: 0.9131\n",
      "Epoch 9/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2185 - accuracy: 0.9154\n",
      "Epoch 10/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2149 - accuracy: 0.9159\n",
      "Epoch 11/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2117 - accuracy: 0.9176\n",
      "Epoch 12/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2088 - accuracy: 0.9187\n",
      "Epoch 13/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2063 - accuracy: 0.9198\n",
      "Epoch 14/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2055 - accuracy: 0.9196\n",
      "Epoch 15/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2042 - accuracy: 0.9196\n",
      "Epoch 16/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.2025 - accuracy: 0.9206\n",
      "Epoch 17/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.1994 - accuracy: 0.9224\n",
      "Epoch 18/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.1977 - accuracy: 0.9226\n",
      "Epoch 19/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.1966 - accuracy: 0.9227\n",
      "Epoch 20/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.1933 - accuracy: 0.9246\n",
      "Epoch 21/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.1921 - accuracy: 0.9245\n",
      "Epoch 22/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.1911 - accuracy: 0.9249\n",
      "Epoch 23/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.1892 - accuracy: 0.9251\n",
      "Epoch 24/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.1873 - accuracy: 0.9254\n",
      "Epoch 25/25\n",
      "967/967 [==============================] - 1s 1ms/step - loss: 0.1849 - accuracy: 0.9271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4bce94da30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = embeddings[test.index].detach().numpy()\n",
    "y_test = test['is_ccp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93      3910\n",
      "         1.0       0.93      0.92      0.93      3820\n",
      "\n",
      "    accuracy                           0.93      7730\n",
      "   macro avg       0.93      0.93      0.93      7730\n",
      "weighted avg       0.93      0.93      0.93      7730\n",
      "\n",
      "[[3661  249]\n",
      " [ 318 3502]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_bool = np.where(y_pred >= 0.5, 1, 0).ravel() #DIY function to round outputs to 0 or 1\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "print(confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 822us/step - loss: 0.1804 - accuracy: 0.9266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1804370880126953, 0.9266493916511536]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9439023]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one line tester\n",
    "\n",
    "line = \"xinjiang is a slavery concentration camp\"\n",
    "line_token = tokenizer(line, padding='max_length', max_length=130, return_tensors=\"pt\")\n",
    "line_embed = bertweet(**line_token)\n",
    "\n",
    "model.predict(line_embed.pooler_output.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on other set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_all = torch.Tensor()\n",
    "\n",
    "for i in range(39):\n",
    "    filename = \"embeddings/all_china_embedding_\" + str(i*1000) + \".pt\"\n",
    "    embeddings = torch.cat((embeddings, torch.load(filename)))\n",
    "    \n",
    "print(embeddings.shape, tweets.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
