{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2491f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b084ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52ec6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../bertweet_embeddings/all_china_full_nort.csv')\n",
    "tweets = tweets.dropna() # some rows come in as blank so they need to be dropped\n",
    "train, test = train_test_split(tweets, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3075ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell tests for invalid or blank text entries and prints their index if one comes up\n",
    "\n",
    "# for idx, tweet in enumerate(text):\n",
    "#     try: \n",
    "#         tokenizer(tweet, padding='max_length', max_length=150, return_tensors=\"pt\")\n",
    "#     except:\n",
    "#         print(idx, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ddd709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38733, 768]) (38733, 3)\n"
     ]
    }
   ],
   "source": [
    "# reading in tensors from file\n",
    "\n",
    "embeddings = torch.Tensor()\n",
    "\n",
    "for i in range(39):\n",
    "    filename = \"../bertweet_embeddings/embeddings/all_china_embedding_\" + str(i*1000) + \".pt\"\n",
    "    embeddings = torch.cat((embeddings, torch.load(filename)))\n",
    "    \n",
    "print(embeddings.shape, tweets.shape) # these should be in agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9d7fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30986, 768) (30986,)\n"
     ]
    }
   ],
   "source": [
    "X_train = embeddings[train.index].detach().numpy()\n",
    "y_train = train['is_ccp']\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "394341d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "#model.add(keras.layers.Normalization())\n",
    "\n",
    "model.add(keras.layers.Dense(16, \n",
    "                             activation = 'relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "#optimizer = keras.optimizers.Adam(clipvalue=1, learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5aa13db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "969/969 [==============================] - 3s 1ms/step - loss: 0.3359 - accuracy: 0.8653\n",
      "Epoch 2/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.2428 - accuracy: 0.9077\n",
      "Epoch 3/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.2202 - accuracy: 0.9175\n",
      "Epoch 4/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.2098 - accuracy: 0.9219\n",
      "Epoch 5/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.2044 - accuracy: 0.9249\n",
      "Epoch 6/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1989 - accuracy: 0.9269\n",
      "Epoch 7/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1941 - accuracy: 0.9279\n",
      "Epoch 8/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1924 - accuracy: 0.9281\n",
      "Epoch 9/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1873 - accuracy: 0.9309\n",
      "Epoch 10/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1840 - accuracy: 0.9317\n",
      "Epoch 11/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1807 - accuracy: 0.9337\n",
      "Epoch 12/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.9351\n",
      "Epoch 13/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1733 - accuracy: 0.9358\n",
      "Epoch 14/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.9357\n",
      "Epoch 15/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1680 - accuracy: 0.9376\n",
      "Epoch 16/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1670 - accuracy: 0.9374\n",
      "Epoch 17/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1617 - accuracy: 0.9395\n",
      "Epoch 18/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1612 - accuracy: 0.9389\n",
      "Epoch 19/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1578 - accuracy: 0.9420\n",
      "Epoch 20/20\n",
      "969/969 [==============================] - 1s 1ms/step - loss: 0.1550 - accuracy: 0.9420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6f76588910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df861659",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d211524",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = embeddings[test.index].detach().numpy()\n",
    "y_test = test['is_ccp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18495ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.84      0.90      3836\n",
      "         1.0       0.86      0.98      0.92      3911\n",
      "\n",
      "    accuracy                           0.91      7747\n",
      "   macro avg       0.92      0.91      0.91      7747\n",
      "weighted avg       0.92      0.91      0.91      7747\n",
      "\n",
      "[[3209  627]\n",
      " [  68 3843]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_bool = np.where(y_pred >= 0.5, 1, 0).ravel() #DIY function to round outputs to 0 or 1\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "print(confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76271974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243/243 [==============================] - 0s 855us/step - loss: 0.2079 - accuracy: 0.9103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20789873600006104, 0.9102878570556641]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b8b467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9234344]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one line tester\n",
    "\n",
    "line = \"xinjiang is a slavery concentration camp\"\n",
    "line_token = tokenizer(line, padding='max_length', max_length=130, return_tensors=\"pt\")\n",
    "line_embed = bertweet(**line_token)\n",
    "\n",
    "model.predict(line_embed.pooler_output.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f3786",
   "metadata": {},
   "source": [
    "# Test on Pro China set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978e656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pro = pd.read_csv('../bertweet_embeddings/pro_china_full_nort.csv')\n",
    "tweets_pro = tweets_pro.dropna() # some rows come in as blank so they need to be dropped\n",
    "xxx, test_pro = train_test_split(tweets_pro, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e30cdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38734, 768]) (38733, 3)\n"
     ]
    }
   ],
   "source": [
    "embeddings_pro = torch.Tensor()\n",
    "\n",
    "for i in range(39):\n",
    "    filename = \"../bertweet_embeddings/embeddings/pro_china_embedding_\" + str(i*1000) + \".pt\"\n",
    "    embeddings_pro = torch.cat((embeddings_pro, torch.load(filename)))\n",
    "    \n",
    "print(embeddings_pro.shape, tweets_pro.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b4f4e3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_pro' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99f2daa9307b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_pro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_pro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_test_pro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_ccp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_pro' is not defined"
     ]
    }
   ],
   "source": [
    "X_test_pro = embeddings_pro[test_pro.index].detach().numpy()\n",
    "y_test_pro = test_pro['is_ccp']\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred_pro = model.predict(X_test_pro)\n",
    "y_pred_pro_bool = np.where(y_pred_pro >= 0.5, 1, 0).ravel() #DIY function to round outputs to 0 or 1\n",
    "\n",
    "print(classification_report(y_test_pro, y_pred_pro_bool))\n",
    "print(confusion_matrix(y_test_pro, y_pred_pro_bool))\n",
    "\n",
    "model.evaluate(X_test_pro, y_test_pro)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1014619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
