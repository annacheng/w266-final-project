{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2491f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99b084ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52ec6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('all_china_full.csv')\n",
    "tweets = tweets.dropna() # some rows come in as blank so they need to be dropped\n",
    "train, test = train_test_split(tweets, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3075ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell tests for invalid or blank text entries and prints their index if one comes up\n",
    "\n",
    "for idx, tweet in enumerate(text):\n",
    "    try: \n",
    "        tokenizer(tweet, padding='max_length', max_length=150, return_tensors=\"pt\")\n",
    "    except:\n",
    "        print(idx, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abf61ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tokenize_and_embed(tweets, batch_size = 5):\n",
    "    \n",
    "    embeddings = torch.Tensor()\n",
    "    \n",
    "    for i in range(0, len(tweets), batch_size):\n",
    "        batch = tweets[i : min(len(tweets), i+batch_size)]\n",
    "        print(\"Processing chunk \" + str(i) + \" to \" + str(i + len(batch)))\n",
    "        \n",
    "        tokens = tokenizer(batch, padding='max_length', max_length=130, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bertweet(**tokens)\n",
    "    \n",
    "        embeddings = torch.cat((embeddings, outputs.pooler_output)) #pooler_output is an embedding for the entire tweet\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42a6ed22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0 to 5\n",
      "Processing chunk 5 to 10\n",
      "Processing chunk 10 to 15\n",
      "Processing chunk 15 to 20\n",
      "Processing chunk 20 to 25\n",
      "Processing chunk 25 to 30\n",
      "Processing chunk 30 to 35\n",
      "Processing chunk 35 to 40\n",
      "Processing chunk 40 to 45\n",
      "Processing chunk 45 to 50\n",
      "Processing chunk 50 to 55\n",
      "Processing chunk 55 to 60\n",
      "Processing chunk 60 to 65\n",
      "Processing chunk 65 to 70\n",
      "Processing chunk 70 to 75\n",
      "Processing chunk 75 to 80\n",
      "Processing chunk 80 to 85\n",
      "Processing chunk 85 to 90\n",
      "Processing chunk 90 to 95\n",
      "Processing chunk 95 to 100\n",
      "Processing chunk 100 to 105\n",
      "Processing chunk 105 to 110\n",
      "Processing chunk 110 to 115\n",
      "Processing chunk 115 to 120\n",
      "Processing chunk 120 to 125\n",
      "Processing chunk 125 to 130\n",
      "Processing chunk 130 to 135\n",
      "Processing chunk 135 to 140\n",
      "Processing chunk 140 to 145\n",
      "Processing chunk 145 to 150\n",
      "Processing chunk 150 to 155\n",
      "Processing chunk 155 to 160\n",
      "Processing chunk 160 to 165\n",
      "Processing chunk 165 to 170\n",
      "Processing chunk 170 to 175\n",
      "Processing chunk 175 to 180\n",
      "Processing chunk 180 to 185\n",
      "Processing chunk 185 to 190\n",
      "Processing chunk 190 to 195\n",
      "Processing chunk 195 to 200\n",
      "Processing chunk 200 to 205\n",
      "Processing chunk 205 to 210\n",
      "Processing chunk 210 to 215\n",
      "Processing chunk 215 to 220\n",
      "Processing chunk 220 to 225\n",
      "Processing chunk 225 to 230\n",
      "Processing chunk 230 to 235\n",
      "Processing chunk 235 to 240\n",
      "Processing chunk 240 to 245\n",
      "Processing chunk 245 to 250\n",
      "Processing chunk 250 to 255\n",
      "Processing chunk 255 to 260\n",
      "Processing chunk 260 to 265\n",
      "Processing chunk 265 to 270\n",
      "Processing chunk 270 to 275\n",
      "Processing chunk 275 to 280\n",
      "Processing chunk 280 to 285\n",
      "Processing chunk 285 to 290\n",
      "Processing chunk 290 to 295\n",
      "Processing chunk 295 to 300\n",
      "Processing chunk 300 to 305\n",
      "Processing chunk 305 to 310\n",
      "Processing chunk 310 to 315\n",
      "Processing chunk 315 to 320\n",
      "Processing chunk 320 to 325\n",
      "Processing chunk 325 to 330\n",
      "Processing chunk 330 to 335\n",
      "Processing chunk 335 to 340\n",
      "Processing chunk 340 to 345\n",
      "Processing chunk 345 to 350\n",
      "Processing chunk 350 to 355\n",
      "Processing chunk 355 to 360\n",
      "Processing chunk 360 to 365\n",
      "Processing chunk 365 to 370\n",
      "Processing chunk 370 to 375\n",
      "Processing chunk 375 to 380\n",
      "Processing chunk 380 to 385\n",
      "Processing chunk 385 to 390\n",
      "Processing chunk 390 to 395\n",
      "Processing chunk 395 to 400\n",
      "Processing chunk 400 to 405\n",
      "Processing chunk 405 to 410\n",
      "Processing chunk 410 to 415\n",
      "Processing chunk 415 to 420\n",
      "Processing chunk 420 to 425\n",
      "Processing chunk 425 to 430\n",
      "Processing chunk 430 to 435\n",
      "Processing chunk 435 to 440\n",
      "Processing chunk 440 to 445\n",
      "Processing chunk 445 to 450\n",
      "Processing chunk 450 to 455\n",
      "Processing chunk 455 to 460\n",
      "Processing chunk 460 to 465\n",
      "Processing chunk 465 to 470\n",
      "Processing chunk 470 to 475\n",
      "Processing chunk 475 to 480\n",
      "Processing chunk 480 to 485\n",
      "Processing chunk 485 to 490\n",
      "Processing chunk 490 to 495\n",
      "Processing chunk 495 to 500\n",
      "Processing chunk 500 to 505\n",
      "Processing chunk 505 to 510\n",
      "Processing chunk 510 to 515\n",
      "Processing chunk 515 to 520\n",
      "Processing chunk 520 to 525\n",
      "Processing chunk 525 to 530\n",
      "Processing chunk 530 to 535\n",
      "Processing chunk 535 to 540\n",
      "Processing chunk 540 to 545\n",
      "Processing chunk 545 to 550\n",
      "Processing chunk 550 to 555\n",
      "Processing chunk 555 to 560\n",
      "Processing chunk 560 to 565\n",
      "Processing chunk 565 to 570\n",
      "Processing chunk 570 to 575\n",
      "Processing chunk 575 to 580\n",
      "Processing chunk 580 to 585\n",
      "Processing chunk 585 to 590\n",
      "Processing chunk 590 to 595\n",
      "Processing chunk 595 to 600\n",
      "Processing chunk 600 to 605\n",
      "Processing chunk 605 to 610\n",
      "Processing chunk 610 to 615\n",
      "Processing chunk 615 to 620\n",
      "Processing chunk 620 to 625\n",
      "Processing chunk 625 to 630\n",
      "Processing chunk 630 to 635\n",
      "Processing chunk 635 to 640\n",
      "Processing chunk 640 to 645\n",
      "Processing chunk 645 to 650\n",
      "Processing chunk 650 to 655\n",
      "Processing chunk 655 to 660\n",
      "Processing chunk 660 to 665\n",
      "Processing chunk 665 to 670\n",
      "Processing chunk 670 to 675\n",
      "Processing chunk 675 to 680\n",
      "Processing chunk 680 to 685\n",
      "Processing chunk 685 to 690\n",
      "Processing chunk 690 to 695\n",
      "Processing chunk 695 to 700\n",
      "Processing chunk 700 to 705\n",
      "Processing chunk 705 to 710\n",
      "Processing chunk 710 to 715\n",
      "Processing chunk 715 to 720\n",
      "Processing chunk 720 to 725\n",
      "Processing chunk 725 to 730\n",
      "Processing chunk 730 to 735\n",
      "Processing chunk 735 to 740\n",
      "Processing chunk 740 to 745\n",
      "Processing chunk 745 to 750\n",
      "Processing chunk 750 to 755\n",
      "Processing chunk 755 to 760\n",
      "Processing chunk 760 to 765\n",
      "Processing chunk 765 to 770\n",
      "Processing chunk 770 to 775\n",
      "Processing chunk 775 to 780\n",
      "Processing chunk 780 to 785\n",
      "Processing chunk 785 to 790\n",
      "Processing chunk 790 to 795\n",
      "Processing chunk 795 to 800\n",
      "Processing chunk 800 to 805\n",
      "Processing chunk 805 to 810\n",
      "Processing chunk 810 to 815\n",
      "Processing chunk 815 to 820\n",
      "Processing chunk 820 to 825\n",
      "Processing chunk 825 to 830\n",
      "Processing chunk 830 to 835\n",
      "Processing chunk 835 to 840\n",
      "Processing chunk 840 to 845\n",
      "Processing chunk 845 to 850\n",
      "Processing chunk 850 to 855\n",
      "Processing chunk 855 to 860\n",
      "Processing chunk 860 to 865\n",
      "Processing chunk 865 to 870\n",
      "Processing chunk 870 to 875\n",
      "Processing chunk 875 to 880\n",
      "Processing chunk 880 to 885\n",
      "Processing chunk 885 to 890\n",
      "Processing chunk 890 to 895\n",
      "Processing chunk 895 to 900\n",
      "Processing chunk 900 to 905\n",
      "Processing chunk 905 to 910\n",
      "Processing chunk 910 to 915\n",
      "Processing chunk 915 to 920\n",
      "Processing chunk 920 to 925\n",
      "Processing chunk 925 to 930\n",
      "Processing chunk 930 to 935\n",
      "Processing chunk 935 to 940\n",
      "Processing chunk 940 to 945\n",
      "Processing chunk 945 to 950\n",
      "Processing chunk 950 to 955\n",
      "Processing chunk 955 to 960\n",
      "Processing chunk 960 to 965\n",
      "Processing chunk 965 to 970\n",
      "Processing chunk 970 to 975\n",
      "Processing chunk 975 to 980\n",
      "Processing chunk 980 to 985\n",
      "Processing chunk 985 to 990\n",
      "Processing chunk 990 to 995\n",
      "Processing chunk 995 to 1000\n",
      "Processing chunk 1000 to 1005\n",
      "Processing chunk 1005 to 1010\n",
      "Processing chunk 1010 to 1015\n",
      "Processing chunk 1015 to 1020\n",
      "Processing chunk 1020 to 1025\n",
      "Processing chunk 1025 to 1030\n",
      "Processing chunk 1030 to 1035\n",
      "Processing chunk 1035 to 1040\n",
      "Processing chunk 1040 to 1045\n",
      "Processing chunk 1045 to 1050\n",
      "Processing chunk 1050 to 1055\n",
      "Processing chunk 1055 to 1060\n",
      "Processing chunk 1060 to 1065\n",
      "Processing chunk 1065 to 1070\n",
      "Processing chunk 1070 to 1075\n",
      "Processing chunk 1075 to 1080\n",
      "Processing chunk 1080 to 1085\n",
      "Processing chunk 1085 to 1090\n",
      "Processing chunk 1090 to 1095\n",
      "Processing chunk 1095 to 1100\n",
      "Processing chunk 1100 to 1105\n",
      "Processing chunk 1105 to 1110\n",
      "Processing chunk 1110 to 1115\n",
      "Processing chunk 1115 to 1120\n",
      "Processing chunk 1120 to 1125\n",
      "Processing chunk 1125 to 1130\n",
      "Processing chunk 1130 to 1135\n",
      "Processing chunk 1135 to 1140\n",
      "Processing chunk 1140 to 1145\n",
      "Processing chunk 1145 to 1150\n",
      "Processing chunk 1150 to 1155\n",
      "Processing chunk 1155 to 1160\n",
      "Processing chunk 1160 to 1165\n",
      "Processing chunk 1165 to 1170\n",
      "Processing chunk 1170 to 1175\n",
      "Processing chunk 1175 to 1180\n",
      "Processing chunk 1180 to 1185\n",
      "Processing chunk 1185 to 1190\n",
      "Processing chunk 1190 to 1195\n",
      "Processing chunk 1195 to 1200\n",
      "Processing chunk 1200 to 1205\n",
      "Processing chunk 1205 to 1210\n",
      "Processing chunk 1210 to 1215\n",
      "Processing chunk 1215 to 1220\n",
      "Processing chunk 1220 to 1225\n",
      "Processing chunk 1225 to 1230\n",
      "Processing chunk 1230 to 1235\n",
      "Processing chunk 1235 to 1240\n",
      "Processing chunk 1240 to 1245\n",
      "Processing chunk 1245 to 1250\n",
      "Processing chunk 1250 to 1255\n",
      "Processing chunk 1255 to 1260\n",
      "Processing chunk 1260 to 1265\n",
      "Processing chunk 1265 to 1270\n",
      "Processing chunk 1270 to 1275\n",
      "Processing chunk 1275 to 1280\n",
      "Processing chunk 1280 to 1285\n",
      "Processing chunk 1285 to 1290\n",
      "Processing chunk 1290 to 1295\n",
      "Processing chunk 1295 to 1300\n",
      "Processing chunk 1300 to 1305\n",
      "Processing chunk 1305 to 1310\n",
      "Processing chunk 1310 to 1315\n",
      "Processing chunk 1315 to 1320\n",
      "Processing chunk 1320 to 1325\n",
      "Processing chunk 1325 to 1330\n",
      "Processing chunk 1330 to 1335\n",
      "Processing chunk 1335 to 1340\n",
      "Processing chunk 1340 to 1345\n",
      "Processing chunk 1345 to 1350\n",
      "Processing chunk 1350 to 1355\n",
      "Processing chunk 1355 to 1360\n",
      "Processing chunk 1360 to 1365\n",
      "Processing chunk 1365 to 1370\n",
      "Processing chunk 1370 to 1375\n",
      "Processing chunk 1375 to 1380\n",
      "Processing chunk 1380 to 1385\n",
      "Processing chunk 1385 to 1390\n",
      "Processing chunk 1390 to 1395\n",
      "Processing chunk 1395 to 1400\n",
      "Processing chunk 1400 to 1405\n",
      "Processing chunk 1405 to 1410\n",
      "Processing chunk 1410 to 1415\n",
      "Processing chunk 1415 to 1420\n",
      "Processing chunk 1420 to 1425\n",
      "Processing chunk 1425 to 1430\n",
      "Processing chunk 1430 to 1435\n",
      "Processing chunk 1435 to 1440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1440 to 1445\n",
      "Processing chunk 1445 to 1450\n",
      "Processing chunk 1450 to 1455\n",
      "Processing chunk 1455 to 1460\n",
      "Processing chunk 1460 to 1465\n",
      "Processing chunk 1465 to 1470\n",
      "Processing chunk 1470 to 1475\n",
      "Processing chunk 1475 to 1480\n",
      "Processing chunk 1480 to 1485\n",
      "Processing chunk 1485 to 1490\n",
      "Processing chunk 1490 to 1495\n",
      "Processing chunk 1495 to 1500\n",
      "Processing chunk 1500 to 1505\n",
      "Processing chunk 1505 to 1510\n",
      "Processing chunk 1510 to 1515\n",
      "Processing chunk 1515 to 1520\n",
      "Processing chunk 1520 to 1525\n",
      "Processing chunk 1525 to 1530\n",
      "Processing chunk 1530 to 1535\n",
      "Processing chunk 1535 to 1540\n",
      "Processing chunk 1540 to 1545\n",
      "Processing chunk 1545 to 1550\n",
      "Processing chunk 1550 to 1555\n",
      "Processing chunk 1555 to 1560\n",
      "Processing chunk 1560 to 1565\n",
      "Processing chunk 1565 to 1570\n",
      "Processing chunk 1570 to 1575\n",
      "Processing chunk 1575 to 1580\n",
      "Processing chunk 1580 to 1585\n",
      "Processing chunk 1585 to 1590\n",
      "Processing chunk 1590 to 1595\n",
      "Processing chunk 1595 to 1600\n",
      "Processing chunk 1600 to 1605\n",
      "Processing chunk 1605 to 1610\n",
      "Processing chunk 1610 to 1615\n",
      "Processing chunk 1615 to 1620\n",
      "Processing chunk 1620 to 1625\n",
      "Processing chunk 1625 to 1630\n",
      "Processing chunk 1630 to 1635\n",
      "Processing chunk 1635 to 1640\n",
      "Processing chunk 1640 to 1645\n",
      "Processing chunk 1645 to 1650\n",
      "Processing chunk 1650 to 1655\n",
      "Processing chunk 1655 to 1660\n",
      "Processing chunk 1660 to 1665\n",
      "Processing chunk 1665 to 1670\n",
      "Processing chunk 1670 to 1675\n",
      "Processing chunk 1675 to 1680\n",
      "Processing chunk 1680 to 1685\n",
      "Processing chunk 1685 to 1690\n",
      "Processing chunk 1690 to 1695\n",
      "Processing chunk 1695 to 1700\n",
      "Processing chunk 1700 to 1705\n",
      "Processing chunk 1705 to 1710\n",
      "Processing chunk 1710 to 1715\n",
      "Processing chunk 1715 to 1720\n",
      "Processing chunk 1720 to 1725\n",
      "Processing chunk 1725 to 1730\n",
      "Processing chunk 1730 to 1735\n",
      "Processing chunk 1735 to 1740\n",
      "Processing chunk 1740 to 1745\n",
      "Processing chunk 1745 to 1750\n",
      "Processing chunk 1750 to 1755\n",
      "Processing chunk 1755 to 1760\n",
      "Processing chunk 1760 to 1765\n",
      "Processing chunk 1765 to 1770\n",
      "Processing chunk 1770 to 1775\n",
      "Processing chunk 1775 to 1780\n",
      "Processing chunk 1780 to 1785\n",
      "Processing chunk 1785 to 1790\n",
      "Processing chunk 1790 to 1795\n",
      "Processing chunk 1795 to 1800\n",
      "Processing chunk 1800 to 1805\n",
      "Processing chunk 1805 to 1810\n",
      "Processing chunk 1810 to 1815\n",
      "Processing chunk 1815 to 1820\n",
      "Processing chunk 1820 to 1825\n",
      "Processing chunk 1825 to 1830\n",
      "Processing chunk 1830 to 1835\n",
      "Processing chunk 1835 to 1840\n",
      "Processing chunk 1840 to 1845\n",
      "Processing chunk 1845 to 1850\n",
      "Processing chunk 1850 to 1855\n",
      "Processing chunk 1855 to 1860\n",
      "Processing chunk 1860 to 1865\n",
      "Processing chunk 1865 to 1870\n",
      "Processing chunk 1870 to 1875\n",
      "Processing chunk 1875 to 1880\n",
      "Processing chunk 1880 to 1885\n",
      "Processing chunk 1885 to 1890\n",
      "Processing chunk 1890 to 1895\n",
      "Processing chunk 1895 to 1900\n",
      "Processing chunk 1900 to 1905\n",
      "Processing chunk 1905 to 1910\n",
      "Processing chunk 1910 to 1915\n",
      "Processing chunk 1915 to 1920\n",
      "Processing chunk 1920 to 1925\n",
      "Processing chunk 1925 to 1930\n",
      "Processing chunk 1930 to 1935\n",
      "Processing chunk 1935 to 1940\n",
      "Processing chunk 1940 to 1945\n",
      "Processing chunk 1945 to 1950\n",
      "Processing chunk 1950 to 1955\n",
      "Processing chunk 1955 to 1960\n",
      "Processing chunk 1960 to 1965\n",
      "Processing chunk 1965 to 1970\n",
      "Processing chunk 1970 to 1975\n",
      "Processing chunk 1975 to 1980\n",
      "Processing chunk 1980 to 1985\n",
      "Processing chunk 1985 to 1990\n",
      "Processing chunk 1990 to 1995\n",
      "Processing chunk 1995 to 2000\n"
     ]
    }
   ],
   "source": [
    "train_subset_size = 2000\n",
    "\n",
    "train_embeddings = batch_tokenize_and_embed(list(train['text'][:train_subset_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ddd709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_embeddings.detach().numpy()\n",
    "y_train = train['is_ccp'][:train_subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "394341d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = keras.Sequential([\n",
    "    # Dense is an affine (xW + b) layer followed by an element wise nonlinearity.\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(4, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# adam optimizer is a fancier version of gradient descent.  You can read more about it here: https://arxiv.org/pdf/1412.6980.pdf\n",
    "linear_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5aa13db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 0.6326 - accuracy: 0.6125\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.7390\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.8150\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4520 - accuracy: 0.8355\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.8615\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8660\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.8645\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8775\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8945\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8755\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8865\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3579 - accuracy: 0.8810\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.9065\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.9035\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.9030\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8875\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.9000\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.9140\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.9100\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.9070\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2836 - accuracy: 0.9115\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.9000\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.9195\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.9245\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2569 - accuracy: 0.9275\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.9245\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.9275\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.9250\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2563 - accuracy: 0.9230\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9325\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2204 - accuracy: 0.9370\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2296 - accuracy: 0.9310\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2182 - accuracy: 0.9340\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2229 - accuracy: 0.9300\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2074 - accuracy: 0.9395\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2002 - accuracy: 0.9440\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9360\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1939 - accuracy: 0.9455\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1866 - accuracy: 0.9465\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1863 - accuracy: 0.9470\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1877 - accuracy: 0.9485\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1892 - accuracy: 0.9495\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1880 - accuracy: 0.9440\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1872 - accuracy: 0.9445\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1885 - accuracy: 0.9415\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1619 - accuracy: 0.9580\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1615 - accuracy: 0.9540\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1556 - accuracy: 0.9610\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1525 - accuracy: 0.9590\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.1663 - accuracy: 0.9510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd442b855e0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df861659",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d211524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0 to 5\n",
      "Processing chunk 5 to 10\n",
      "Processing chunk 10 to 15\n",
      "Processing chunk 15 to 20\n",
      "Processing chunk 20 to 25\n",
      "Processing chunk 25 to 30\n",
      "Processing chunk 30 to 35\n",
      "Processing chunk 35 to 40\n",
      "Processing chunk 40 to 45\n",
      "Processing chunk 45 to 50\n",
      "Processing chunk 50 to 55\n",
      "Processing chunk 55 to 60\n",
      "Processing chunk 60 to 65\n",
      "Processing chunk 65 to 70\n",
      "Processing chunk 70 to 75\n",
      "Processing chunk 75 to 80\n",
      "Processing chunk 80 to 85\n",
      "Processing chunk 85 to 90\n",
      "Processing chunk 90 to 95\n",
      "Processing chunk 95 to 100\n",
      "Processing chunk 100 to 105\n",
      "Processing chunk 105 to 110\n",
      "Processing chunk 110 to 115\n",
      "Processing chunk 115 to 120\n",
      "Processing chunk 120 to 125\n",
      "Processing chunk 125 to 130\n",
      "Processing chunk 130 to 135\n",
      "Processing chunk 135 to 140\n",
      "Processing chunk 140 to 145\n",
      "Processing chunk 145 to 150\n",
      "Processing chunk 150 to 155\n",
      "Processing chunk 155 to 160\n",
      "Processing chunk 160 to 165\n",
      "Processing chunk 165 to 170\n",
      "Processing chunk 170 to 175\n",
      "Processing chunk 175 to 180\n",
      "Processing chunk 180 to 185\n",
      "Processing chunk 185 to 190\n",
      "Processing chunk 190 to 195\n",
      "Processing chunk 195 to 200\n",
      "Processing chunk 200 to 205\n",
      "Processing chunk 205 to 210\n",
      "Processing chunk 210 to 215\n",
      "Processing chunk 215 to 220\n",
      "Processing chunk 220 to 225\n",
      "Processing chunk 225 to 230\n",
      "Processing chunk 230 to 235\n",
      "Processing chunk 235 to 240\n",
      "Processing chunk 240 to 245\n",
      "Processing chunk 245 to 250\n",
      "Processing chunk 250 to 255\n",
      "Processing chunk 255 to 260\n",
      "Processing chunk 260 to 265\n",
      "Processing chunk 265 to 270\n",
      "Processing chunk 270 to 275\n",
      "Processing chunk 275 to 280\n",
      "Processing chunk 280 to 285\n",
      "Processing chunk 285 to 290\n",
      "Processing chunk 290 to 295\n",
      "Processing chunk 295 to 300\n",
      "Processing chunk 300 to 305\n",
      "Processing chunk 305 to 310\n",
      "Processing chunk 310 to 315\n",
      "Processing chunk 315 to 320\n",
      "Processing chunk 320 to 325\n",
      "Processing chunk 325 to 330\n",
      "Processing chunk 330 to 335\n",
      "Processing chunk 335 to 340\n",
      "Processing chunk 340 to 345\n",
      "Processing chunk 345 to 350\n",
      "Processing chunk 350 to 355\n",
      "Processing chunk 355 to 360\n",
      "Processing chunk 360 to 365\n",
      "Processing chunk 365 to 370\n",
      "Processing chunk 370 to 375\n",
      "Processing chunk 375 to 380\n",
      "Processing chunk 380 to 385\n",
      "Processing chunk 385 to 390\n",
      "Processing chunk 390 to 395\n",
      "Processing chunk 395 to 400\n",
      "Processing chunk 400 to 405\n",
      "Processing chunk 405 to 410\n",
      "Processing chunk 410 to 415\n",
      "Processing chunk 415 to 420\n",
      "Processing chunk 420 to 425\n",
      "Processing chunk 425 to 430\n",
      "Processing chunk 430 to 435\n",
      "Processing chunk 435 to 440\n",
      "Processing chunk 440 to 445\n",
      "Processing chunk 445 to 450\n",
      "Processing chunk 450 to 455\n",
      "Processing chunk 455 to 460\n",
      "Processing chunk 460 to 465\n",
      "Processing chunk 465 to 470\n",
      "Processing chunk 470 to 475\n",
      "Processing chunk 475 to 480\n",
      "Processing chunk 480 to 485\n",
      "Processing chunk 485 to 490\n",
      "Processing chunk 490 to 495\n",
      "Processing chunk 495 to 500\n"
     ]
    }
   ],
   "source": [
    "test_subset_size = 500\n",
    "\n",
    "test_embeddings = batch_tokenize_and_embed(list(test['text'][:test_subset_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b18495ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_embeddings.detach().numpy()\n",
    "y_test = test['is_ccp'][:test_subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76271974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.98      0.91       208\n",
      "         1.0       0.98      0.88      0.93       292\n",
      "\n",
      "    accuracy                           0.92       500\n",
      "   macro avg       0.92      0.93      0.92       500\n",
      "weighted avg       0.93      0.92      0.92       500\n",
      "\n",
      "[[203   5]\n",
      " [ 35 257]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = linear_model.predict(X_test)\n",
    "y_pred_bool = np.where(y_pred >= 0.5, 1, 0).ravel() #DIY function to round outputs to 0 or 1\n",
    "\n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "print(confusion_matrix(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6ed2b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23062360286712646, 0.9200000166893005]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.evaluate(X_test, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59b8b467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_pred >= 0.5, 1, 0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53867005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33969    1.0\n",
       "1117     1.0\n",
       "17702    1.0\n",
       "9627     1.0\n",
       "45866    0.0\n",
       "        ... \n",
       "36334    1.0\n",
       "37655    1.0\n",
       "34140    1.0\n",
       "13758    1.0\n",
       "55876    0.0\n",
       "Name: is_ccp, Length: 500, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
